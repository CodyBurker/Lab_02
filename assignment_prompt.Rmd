---
title: "Lab 2: Regression to Study the Spread of Covid-19"
author: 'w203: Statistics for Data Science'
date: "10/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this lab, you will apply what you are learning about linear regression to study the spread of COVID-19.  Your task is to select a research question, then conduct a regression study to analyize it. 

Your research question must focus attention on a specific measurement goal.  It is not enough to say "we are looking for policies that help stop COVID-19" (That would be a fishing expedition).  Instead, use your introduction to motivate a specific effect for measurment. 

Once you have a measurement goal, you will build a set of linear models that are tailored to that goal, and display them in a well formatted regression table.  You will finally use your conclusion to distill important conclusions from your estimates.

This is a group assignment.  Your live session instructor will coordinate the formation of groups.

When working in a group, do not use a "division-of-labor" approach to complete the lab.  All students should participate in all aspects of the final report.  We believe that this will maximize the value you derive from the lab.

# Timeline

The lab takes place in three stages.

- **Stage 1: Draft Report.**  You will create a draft report.
- **Stage 2: Peer Feedback.**  Teams will exchange reports and provide each other with feedback.
- **Stage 3: Final Report.**  You will incorporate the feedback you receive and create a final report.

# Submission Instructions

- Please submit your answers in *one* PDF and *one* .Rmd file. 
- Only your answers in the PDF document will be considered for grading. The notebook is required to verify that any scripts that you have written can be executed.
- You are allowed to submit supplementary files such as images of handwritten notes imported into your notebook. Note, however, that no handwritten notes will be considered for grading.

# The Data

The data is provided in a file within this repository.  Majid Maki-Nayeri compiled the data, drawing many variables from the COVID-19 US state policy database (Raifman J, Nocka K, Jones D, Bor J, Lipson S, Jay J, and Chan P.).

- The data is available [here](https://mids-w203.s3-us-west-1.amazonaws.com/covid-19_dist0720+.xlsx). 
- Supporting documents like the specific legal language used by states, additional data sources, and much more are available in unstructured format [here](https://tinyurl.com/statepolicysources).

The dataset includes: 

1. Variables representing the spread of the disease; 
2. Variables representing state-level policy responses; and, 
3. General state-level characteristics.

If you want to, you are allowed to add extra variables from external sources.  However, this is not necessary, and we will not assign any bonus points to teams that derive unique data, as our focus is on statistics and statistical writing.

# Stage 1: Draft Report - Due at Live Session 12

In the first stage of the lab, you will create a draft report.  You should aim to make this report as complete as possible, so that you get the best possible feedback in stage 2.  (If you have extra time during these weeks, it will be beneficial to watch ahead in the async videos, as far as unit 12)

The exact format of your report is flexible, but it must include the following elements:

## 1. An Introduction

Your introduction should present a research question and explain the concept that you're attempting to measure and how it will be operationalized.  This section should pave the way for the body of the report, preparing the reader to understand why the models are constructed the way that they are.  It is not enough to simply say "We are looking for policies that help against covid."  Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propelling the narrative forward.  This is also good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data.

## 2. A Model Building Process

You will next build a set of models to investigate your research question, documenting your decisions.  Here are some things to keep in mind during your model building process:

1. *What do you want to measure*?  Make sure you identify one, or a few, variables that will allow you to derive conclusions relevant to your research question, and include those variables in all model specifications.
2. Is your modeling goal one of description or explaination? 
3. What [covariates](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms) help you achieve your modeling goals?  What covariates are problematic, either due to *multicollinearity*, or because they will absorb some of a causal effect you want to measure?
4. What *transformations*, if any, should you apply to each variable?  These transformations might reveal linearities in the data, make our results relevant, or help us meet model assumptions.
5. Are your choices supported by exploratory data analysis (*EDA*)?  You will likely start with some general EDA to *detect anomalies* (missing values, top-coded variables, etc.).  From then on, your EDA should be interspersed with your model building.  Use visual tools to *guide* your decisions.  You can also leverage statistical *tests* to help assess whether variables, or groups of variables, are improving model fit.

At the same time, it is important to remember that you are not trying to create one perfect model.  You will create several specifications, giving the reader a sense of how robust (or sensitive) your results are to modeling choices, and to show that you're not just cherry-picking the specification that leads to the largest effects.

At a minimum, you should include the following three specifications:

1. **Baseline, version 0 (v0)**: One model with *only the key variables* you want to measure (possibly transformed, as determined by your EDA), and no other covariates (or perhaps one, or at most two, covariates if they are so crucial that it would be unreasonable to omit them)
1. **Improvement, v1**: One model that includes *key explanatory variables and covariates that you believe advance your modeling* goals without introducing too much multicollinearity or causing other issues.  This model should strike a balance between accuracy and parsimony and reflect your best understanding of the relationships among key variables.
1. **Improvement, v2**: One model that includes the *previous covariates, and many other covariates*, erring on the side of inclusion.  A key purpose of this model is to demonstrate the robustness of your results to model specification.  (However, you should still not include variables that are clearly unreasonable. For example, don't include outcome variables that will absorb some of the causal effect you are interested in measuring.)

Guided by your background knowledge and your EDA, other specifications may make sense.  You are trying to choose points that encircle the space of reasonable modeling choices, to give an overall understanding of how these choices impact results.

## 3. Limitations of your Model 

As a team, evaluate all of the CLM assumptions that must hold for your model. However, do not report an exhaustive examination all 5 CLM assumption. Instead, bring forward only those assumptions that you think pose significant problems for your analysis. For each problem that you identify, describe the statistical consequences. If you are able to identify any strategies to mitigate the consequences, explain these strategies. 

Note that you may need to change your model specifications in response to violations of the CLM. 

## 4. A Regression Table

You should display all of your model specifications in a regression table, using a package like [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) to format your output.  It should be easy for the reader to find the coefficients that represent key effects near the top of the regression table, and scan horizontally to see how they change from specification to specification.  Make sure that you display the most appropriate standard errors in your table, along with significance stars.

In your text, comment on both *statistical significance and practical significance*.  You may want to include statistical tests besides the standard t-tests for regression coefficients.

## 5. Discussion of Omitted Variables

If the team has taken up an explanatory (i.e. causal) question to evaluate, then identify what you think are the 5 most important *omitted variables* that bias results you care about.  For each variable, you should *reason about the direction of bias* caused by omitting this variable.  If you can argue whether the bias is large or small, that is even better.  State whether you have any variables available that may proxy (even imperfectly) for the omitted variable. Pay particular attention to whether each omitted variable bias is *towards zero or away from zero*.  You will use this information to judge whether the effects you find are likely to be real, or whether they might be entirely an artifact of omitted variable bias.

## 6. Conclusion

Make sure that you end your report with a discussion that distills key-takeaways from your estimates, addresses your research question, and draws attention to larger contexts.

## Submission

- Submit your lab via ISVC; please do not submit via email.
- Submit 2 files:
    1. A pdf file including the summary, the details of your analysis, and all the R codes used to produce the analysis. **Please do not suppress the code in your pdf file.**
    2. The Rmd or ipynb source file used to produce the pdf file.
- Each group only needs to submit one set of files.
- Be sure to include the names of all team members in your report.  Place the word 'draft' in the file names.
- Please limit your submission to 8000 words, excluding code cells and R output.


# Stage 2: Peer Feedback - Due at Live Session 13

In Stage 2, you will provide feedback on another team's draft report.  We will ask you to comment separately on different sections.  The following list is very similar to the rubric we will use when grading your final report.

- **Introduction.**  Is the introduction clear? Is the research question specific and well defined?  Does the introduction motivate a specific concept to be measured and explain how it will be operationalized.  Does it do a good job of preparing the reader to understand the model specifications?

- **The Initial Data Loading and Cleaning.**  Did the team notice any anomalous values?  Is there a sufficient justification for any data points that are removed?  Did the report note any coding features that affect the meaning of variables (e.g. top-coding or bottom-coding)?  Overall, does the report demonstrate a thorough understanding of the data?

- **The Model Building Process.** Overall, is each step in the model building process supported by EDA?  Is the outcome variable (or variables) appropriate? Is there a thorough univariate analysis of the outcome variable? Did the team identify one, or a few, explanatory variables and perform a thorough univariate analysis of each one? Did the team clearly state why they chose these explanatory variables, does this explanation make sense in term of their research question? Did the team consider available variable transformations and select them with an eye towards model plausibility and interperability?  Are transformations used to expose linear relationships in scatterplots?  Is there enough explanation in the text to understand the meaning of each visualization?

- **Regression Models:**
    - **Base Model.** Does this model only include key explanatory variables?  Do the variables make sense given the measurement goals?  Did the team apply reasonable transformations to these variables, to capture the nature of the relationships?
    - **Second Model.** Does this model represent a balanced approach, including variables that advance modeling goals without causing major issues?  Does the model suceed in reducing standard errors of the key variables compared to the base model?  Does it capture major nonlinearities in the joint distribution of the variables?
    - **Third Model.**  Does this model represent a maximalist approach, erring on the side of including most variables?  Is it still a reasonable model?  Are there any variables that are outcomes, and should therefore still be excluded?  Is there too much multicolinearity, to the point that the key causal effects cannot be measured?
    
- **Plots, Figures, and Table** Do the plot, figures and tables that the team has chosen to include successfully move forward the argument that they are making? Do they have a good ratio of "Information to Ink" (Tufte)? Has the team chosen the most effective method (a table or a chart) to display their evidence? IS that table or chart the most communicative it could be? Is every plot, figure of table that is included in the report referenced in the narrative argument?

- **Assessment of the CLM.**  Has the team assessed each of the CLM assumptions (including random sampling)?  Did they use visual tools or statistical tests, as appropriate?  Did they respond appropriately to any violations?

- **A  Regression Table.**  Are the model specifications properly chosen to outline the boundary of reasonable choices?  Is it easy to find key coefficients in the regression table?  Does the text include a discussion of practical significance for key effects? 

- **An Omitted Variables Discussion.**  Did the report miss any important sources of omitted variable bias?  Are the estimated directions of bias correct? Was their explanation clear?  Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?

- **Conclusion.**  Does the conclusion address the research question?  Does it raise interesting points beyond numerical estimates?  Does it place relevant context around the results?

- Are there any other errors, faulty logic, unclear or unpersuasive writing, or other elements that leave you less convinced by the conclusions?

Please be thorough and read the report critically, actively trying to find areas that the report could be improved.  Your comments will directly help your peers get the most value out of the project.

# Stage 3: Final Report - Due at Live Session 14

In the final stage of the project, you will incorporate the feedback you receive, and use what you've learned about OLS inference to create a final report.

We will assess your final report using a rubric that includes the elements listed above.  We will also consider whether you have correctly included elements of statistical inference in your report.

Please limit your submission to 8000 words, excluding code cells and R output.

As above, you must submit both the source and pdf files.  Be sure to include the names of all team members in your report.  Place the word 'final' in the file names.
